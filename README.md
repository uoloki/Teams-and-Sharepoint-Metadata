# Metadata Capture Documentation

## Overview
This documentation provides a guide on how to use two Python scripts for capturing and filtering metadata from Microsoft 365 services. The first script, `List_Capture.py`, retrieves metadata from various Microsoft 365 endpoints and saves it to an Excel file. The second script, `Meta_Data_Capture.py`, reads the Excel file, filters the data based on specified criteria, and saves the filtered data to a new Excel file.

## Prerequisites
- **Python 3.6 or higher**
- **Necessary Python libraries:**
  - `requests`
  - `pandas`
  - `openpyxl`

You can install the required libraries using pip:

```sh
pip install requests pandas openpyxl
```

- **Azure AD Application with appropriate permissions:**
  - `User.Read.All` - Read all users' full profiles.
  - `Group.Read.All` - Read all groups.
  - `Team.ReadBasic.All` - Read the names and descriptions of all channels.
  - `ChannelMessage.Read.All` - Read all channel messages.
  - `Sites.Read.All` - Read items in all site collections.
  - `Channel.ReadBasic.All` - Read the names and descriptions of all channels.
  - `ChannelMember.Read.All` - Read the members of all channels.
  - `GroupMember.Read.All` - Read all group memberships.

## Script 1: `List_Capture.py`

### Purpose
Fetches metadata from Microsoft 365 endpoints (Users, Groups, Teams, Channels, Messages, SharePoint Sites, and Files) and saves it to an Excel file.

### How to Use

1. **Configure a `credentials.txt` file:**

   Place the file in the same directory as the script. The file should contain the following lines with your Azure AD application's credentials:
   
   ```credentials.txt
   TENANT_ID=<your-tenant-id>
   CLIENT_ID=<your-client-id>
   CLIENT_SECRET=<your-client-secret>
   ```

3. **Run the Script:**

   Execute the script from the terminal:

   ```sh
   python List_Capture.py
   ```

### Output
The script will generate an Excel file named `all_metadata.xlsx` with the following sheets:
- Users
- Groups
- Teams
- Channels
- Messages
- Sites
- Files

### Script Explanation
- load_credentials(): Loads Azure AD credentials from the `credentials.txt` file.
- get_access_token(): Obtains an access token using client credentials.
- get_data_from_endpoint(): Fetches data from specified Microsoft Graph API endpoints.
- add_y_columns(): Adds a 'Y' column for each existing column.
- save_to_excel(): Saves data to an Excel file and adjusts column widths.
- adjust_column_width(): Adjusts the column width of the Excel sheet to fit the content.
- fetch_site_drive_items() and fetch_folder_items(): Fetch items from SharePoint sites' document libraries.
- main(): Orchestrates the data fetching and saving process.

## Script 2: `Meta_Data_Capture.py`

### Purpose
Reads the Excel file generated by `List_Capture.py`, filters the data based on the presence of 'Y' in the _Y columns, and saves the filtered data to a new Excel file.

### How to Use

1. **Ensure the First Script Has Run:**

   Make sure the `List_Capture.py` script has run and generated the `all_metadata.xlsx` file.

2. **Run the Script:**

   Execute the script from the terminal:

   ```sh
   python Meta_Data_Capture.py
   ```

### Output
The script will generate a new Excel file named `filtered_metadata.xlsx` containing the filtered data.

### Script Explanation
- load_data(): Loads data from an Excel sheet.
- filter_data(): Filters data based on the presence of 'Y' in the _Y columns.
- save_filtered_data(): Saves the filtered data to a new Excel file.
- main(): Orchestrates the data loading, filtering, and saving process.

## Directory Structure

```bash
.
├── List_Capture.py
├── Meta_Data_Capture.py
├── credentials.txt
└── all_metadata.xlsx  # Generated by the first script
```

## Logging
Both scripts generate log files (`metadata_fetch.log` and `metadata_capture.log`) that contain detailed information about the script's execution, including any errors encountered. These logs can be useful for debugging and verifying that the scripts ran correctly.

## Error Handling
Both scripts include robust error handling to manage common issues, such as missing files, API request failures, and data processing errors. Errors are logged to the respective log files.

